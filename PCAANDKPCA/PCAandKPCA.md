

# PCA

机器学习中，有时会出现这样的情况，在训练样本固定的情况下，特征维数增加到某一个临界点后，继续增加反而导致模型的预测能力减小——这叫做休斯现象



每个现实世界的事物，再用来进行机器学习训练和预测时，需要转化为一个**特征向量**



##### 本征维度

信号的本征维度描述了需要用来表示信号的变量数量。对于含有N个变量的信号而言，它的本征维度为M，M满足0 &le;M&le;N。

本征维度支出，许多高位数据集通过削减维度至低维空间，而不必丢失重要信息。



##### 降维的本质

学习一个映射函数 y = f(x),其中x表示原始的高维数据，y表示映射后的低维数据



##### 线性可分

D<sub>0</sub>和D<sub>1</sub>是n维欧式空间的两个点集，如果存在n维向量&omega;和实数b,使得所有属于D<sub>0</sub>的点x<sub>i</sub>都有&omega;x<sub>i</sub>+b>0,而对于所有属于D<sub>1</sub>的点x<sub>j</sub>则有&omega;x<sub>j</sub>+b<0,则我们称D<sub>0</sub>和D<sub>1</sub>线性可分。

图片：

![线性可分](D:\usegit\github\machinelearningalgor\PCAANDKPCA\图片\线性可分.png)

##### 超平面

n维欧式空间中维度等于n-1的线性子空间

什么样的超平面是最佳的？——以最大间隔将两类样本分开的超平面，是最佳超平面



> 引入：如果能把一个空间的样本点映射到它的超平面上去，这样一来，映射后的结果不就只存在于超平面空间（也就是原空间的子空间）了吗，这样就获得了降维的结果。
>
> 如何保证超平面中的点不丧失对应原始点的“主成分”呢？
>
> * 尽量使源空间中的样本点在超平面上不重叠
> * 必然会丢失信息，直接丢失掉的，就是源空间样本到达超平面的距离。因此，尽量使得这个超平面靠近源空间样本点——如果样本点到超平面的距离只有很小的一段，那么映射后它丢失的信息量也相应地会很小



##### 理想的超平面应具备的两个性质

* **最大可分性**：样本点到这个超平面的投影尽量能够分开；
* **最近重构性**：样本点到这个超平面的距离尽量近；



##### PCA公式推导(默认都是列向量)

假设有 n 个数据样本，这些样本原本属于一个 d 维空间。

先对样本数据做一下中心化，使得 &sum;<sub>i</sub>x<sup>(i)</sup>=0 (整体均值移动至原点)

源空间里的第 i 个样本可以表示为：x<sup>(i)</sup> = (x<sub>1</sub><sup>(i)</sup>，x<sub>2</sub><sup>(i)</sup>，...，x<sub>d</sub><sup>(i)</sup>)

将它投影到一个 d<sup>‘</sup> 维的空间，有d<sup>&rsquo;</sup><d。第 i 样本投射到低维空间后表示为：z<sup>(i)</sup> = (z<sub>1</sub><sup>(i)</sup>,z<sub>2</sub><sup>(i)</sup>,...,z<sub>d‘</sub><sup>(i)</sup>)

从x<sup>(i)</sup>到z<sup>(i)</sup>的转换表示为：
$$
z_j^{(i)} = w_j^Tx^{(i)},(i=1,2,...,n;j=1,2,...,d)
$$
该式中，Z<sub>j</sub><sup>(i)</sup> 表示x<sup>(i)</sup>在低维坐标系下第 j 维的坐标。

&omega;<sub>j</sub>是一个d维的权重向量，&omega;<sub>j</sub>= (&omega;<sub>j1</sub>,&omega;<sub>j2</sub>,...,&omega;<sub>jd</sub>)



如果基于z<sup>(i)</sup>来重构x<sup>(i)</sup>,令：
$$
\hat{x}=\sum _{j=1}^{d^{'}}z_{j}^{(i)}\omega _{j}
-其中\hat{x}是原本d维空间中的样本点x^{i}投影到新的的d^{'}维样本点在原本d维空间中的位置
$$
> >解释一下该式子：
> >$$
> >由于 z_j^{(i)} = w_j^Tx^{(i)}
> >$$
> >
> >$$
> >且降维之后，相当于原点朝降维后的维度空间里做垂线，当前的w只是保留了原变换矩阵的d^‘维度的信息
> >$$
> >
> >

当前目标为：所有n个样本分别与其基于投影重构的样本点间的**距离整体最小**

整体距离为公式：
$$
\sum_{i=1}^{n}||\hat{x}-x^{i}||_2^2
$$
因为：
$$
\sum_{i=1}^{n}||\hat{x}-x^{i}||_2^2=\sum_{i=1}^{n}
$$

