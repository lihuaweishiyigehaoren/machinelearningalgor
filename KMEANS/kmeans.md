- [ ] # KMeans


##### 无监督学习（聚类方法）



##### 聚类算法

聚类算法中，样本的属性主要由其在特征空间中的相对距离来表示，这使得距离的概念对于聚类非常重要。



##### 距离类型

- [ ] 欧式距离 

  点 x = (x<sub>1</sub>，……，x<sub>n</sub>)和 y = （y<sub>1</sub>，……，y<sub>n</sub>）之间的欧式距离为：
  $$
  d(x,y) = \sqrt{(x_1-y_1)^2+……+（x_n-y_n)^2}=\sqrt{\sum_{i=1}^n(x_i-y_i)^2}
  $$

- [ ] 余弦距离（又称余弦相似性）

  两个向量间的余弦值可以通过使用欧几里德点积公式求出：
  $$
  cos(\Theta)= \frac{\sum_{i=1}^nA_i \times B_i}{\sqrt{\sum_{i=1}^n{A_i}^2} \times \sqrt{\sum_{i=1}^n{B_i}^2}}
  $$

  - -1 意味着两个向量指向的方向截然相反；
  - 1 表示他们的指向是完全相同的；
  - 0 表示它们之间是独立的；
  - [-1,1]之间的其他值则表示中间程度的相似性或相异性

- [ ] 曼哈顿距离

  ![曼哈顿距离](D:\usegit\github\machinelearningalgor\KMEANS\图片\曼哈顿距离.png)

​      从点（x<sub>1</sub>,y<sub>1</sub>)到点（x<sub>2</sub>,y<sub>2</sub>)，曼哈顿距离为：
$$
|x_1-y_1|+|y_1-y_2|
$$
当然，还存在其他很多距离表示方式；



##### 定义

KMeans 是一种聚类方法，k 是一个常数值，由使用者指定，这种算法负责将特征空间中的 n 个向量聚集到 k 个簇中。



##### 算法步骤

1. 用户确定 k 值，并将 n 个样本投射为特征空间（一般为欧式空间）中的 n 个点（k<=n)
2. 算法在这 n 个点中随机选取 k 个点，作为初始的“簇核心”
3. 分别计算每个样本点到 k 个簇核心的距离（这里的距离一般取欧氏距离或余弦距离），找到离该点最近的簇核心，将它归属到对应的簇
4. 所有点到归属到簇之后， n 个点就分为了 k 个簇。之后重新计算每个簇的中心（平均距离中心），将其定义为新的“簇核心”
5. 反复迭代步骤3-4，直到簇核心不再移动为止



##### 迭代的目标

KMeans的算法目标是使得簇内的平方和最小：

有 n 个样本（x<sub>1</sub>,x<sub>2</sub>,……，x<sub>n</sub>	)，每个都是 d 维实向量，KMeans聚类的目标是将它们分为 k 个簇(k<=n)，这些簇的表示为S=(S<sub>1</sub>,S<sub>2</sub>,……，S<sub>k</sub>)。
$$
min\sum_{i=1}^{k}\sum_{x \epsilon S_i}||x-u_i||^2 其中u_i是S_i的重心
$$

##### 分配

步骤3:-分配

设此时为时刻t，t时刻S<sub>i</sub>的簇核心为&mu;<sub>i</sub><sup>(t)</sup>.

将某个样本点x<sub>p</sub>归入S<sub>i</sub><sup>(t)</sup>的原则是：它归入该簇后，对该簇WCSS的贡献最小：
$$
S_i^{(t)} = \begin{Bmatrix}{x_p}:||x_p-u_i^{(t)}||^2 \leq ||x_p-u_j^{(t)}||^2 \forall j,1\leq j \leq k \end{Bmatrix}
$$
因为WCSS等于簇中个点到该簇核心的欧氏距离的平方和，又因为在每次进行步骤3之前，我们认定了当时所有簇的簇核心&mu;<sub>i</sub><sup>(t)</sup>,i=1，2，……，k已经存在。

##### 更新

步骤4-更新

这一步要重新求簇核心，具体计算非常简单，对于该簇中的所有样本求均值就好：
$$
u_i^{(t+1)} = \frac {1}{|S_i^{(t)}|} \sum_{x_j \epsilon S_i^{(t)}}x_j   其中|S_i|表示样本个数
$$

##### 启发式算法

是一种基于直观或经验构造的算法。

相对于最优化算法要求得待解决问题的最优解，启发式算法力求在可接受的花费（消耗的时间和空间）下，给出待解决问题的一个可行解，该可行解与最优解的偏离程度一般不能被预计。

启发式算法常能发现不错的解，但也没办法证明它不会得到较坏的解；它通常可在合理时间解出答案，但也没办法知道它是否每次都可以这样的速度求解。

虽然有种种不确定性，且其性能无法得到严格的数学证明，但启发式算法简单、直观、易于实现。

即使在某些特殊情况下，启发式算法会得到很坏的答案或效率极差，然而造成那些特殊情况的数据组合，也许永远不会在现实世界出现。

因此现实世界中启发算法常用来解决问题。



##### LIoyd's算法

该算法是用于实现KMeans的启发式算法，时间复杂度O(nkdi)-n为样本数，k为簇个数,d为样本维度数，而i为开始到收敛的迭代次数。



##### 局限性

- k 值对最终结果的影响至关重要，而它却必须要预先给定。给定合适的k值，需要先验知识，凭空估计很困难，或者可能导致效果很差
- 初始簇核心一般是随机选定的，偏偏它们又很重要，几乎可是说是算法敏感的——一旦选择的不合适，可能只能得到局部的最优解，而无法得到全局的最优解，当然，这也是有KMean算法本身的局部最优性决定的

因而，造成了KMeans的应用局限，使得它不能适合所有的数据。

例如：对于非球形簇，或者多个簇之间尺寸和密度相差较大的情况，KMeans就处理不好了。